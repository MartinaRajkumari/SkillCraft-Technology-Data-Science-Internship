## SkillCraft Data Science Internshipâ€”2025

This repository contains all the tasks that have been completed during my Data Science Internship at SkillCraft Technology. This internship provided experience with real-world datasets and helped in strengthening my understanding of Exploratory Data Analysis and Data Visualization.

## Internship

- **Organization:** SkillCraft Technology  
- **Role:** Data Science Intern  
- **Duration:** 1 July,2025 to 31 July,2025
- **Key Tools & Libraries:**  
  `Python`, `Pandas`, `Matplotlib`, `Seaborn`, `Scikit-learn`, `Folium`, `Jupyter Notebook`

## Tasks Summary

### âœ… Task 01:
- Objective: To create a bar chart to visualize the distribution of a categorical or continuous variable, such as the distribution of ages or genders in a population.

- Key Activities:<br/>
-Explored feature relationships such as:<br/>
    *Population Vs Gender<br/>
    *Population Vs Age group<br/>
Age ranges from (0-14),(15-64) and (65 and above)

- Tools and Libraries used: Python, numpy, matplotlib, pandas

- Key Visuals Created:<br/>
ğŸ“Š India Population by Gender(2000-2024)<br/>
ğŸ“Š India Population by Age Group and Gender(2000-2024)


### âœ… Task 02<br/>
Objective: To perform Data Cleaning and Exploratory Data Analysis (EDA) on the Titanic dataset and uncover relationships between key variables to identify patterns and trends.<br/>

- Key Activities:<br/>
-Loaded and examined the Titanic dataset from Kaggle<br/>
-Cleaned the data by:<br/>
   *Dropping irrelevant columns (Cabin, PassengerId, Name, Ticket)<br/>
   *Filling missing values in Age and Embarked using median and mode respectively<br/>
-Performed EDA using Python libraries<br/>
-Created visualizations to explore trends<br/>

- Tools and Libraries used: Python, pandas, seaborn and matplotlib<br/>

- Key Visuals Created:<br/>
  ğŸ“Š Overall survival distribution<br/>
  ğŸ“Š Survival by gender<br/>
  ğŸ“Š Survival by class<br/>
  ğŸ“Š Survival by embarkment point<br/>
  ğŸ“Š Age distribution among survivors<br/>
  ğŸ“Š Correlation heatmap <br/>
  ğŸ“Š Pairwise relationships between key features in the dataset<br/>

ğŸ“ˆ Outcome:<br/>
Through this task, I strengthened my ability to clean real-world datasets, deal with missing and irrelevant data and apply visual EDA techniques to uncover insights and trends. I practiced using correlation analysis and pairplots to examine patterns and improved my data storytelling using clear and purposeful visualizations.<br/>

### âœ… Task 03<br/>
- Objective: To build a Decision Tree Classifier to predict whether a customer will purchase a product or service based on their demographic and behavioral data using the Bank Marketing dataset.

- Key Activities:<br/>
-Loaded and explored the Bank Marketing dataset from the UCI repository<br/>
-Preprocessed the data by:<br/>
   *Encoding categorical variables using LabelEncoder<br/>
   *Splitting the data into training and test sets<br/>
-Built a Decision Tree Classifier using scikit-learn<br/>
-Evaluated the model using accuracy score and classification report<br/>
-Created visualizations to understand model performance and customer behavior patterns

- Tools and Libraries used: Python, pandas, seaborn, matplotlib, scikit-learn

- Key Visuals Created:<br/>
ğŸ“Š Full decision tree structure showing how predictions are made<br/>
ğŸ“Š Age distribution vs subscription status<br/>
ğŸ“Š Success rate by education level<br/>
ğŸ“Š Feature importance plot to highlight the most influential variables

ğŸ“ˆ Outcome:<br/>
This task helped me understand how decision trees function and how they can be used for classification tasks. I strengthened my skills in model building, feature encoding, evaluation metrics, and visualization. I also learned how to interpret machine learning results to gain meaningful business insights.

### âœ… Task 04<br/>
- Objective: To analyze traffic accident data and identify patterns based on road conditions, weather, and time of day, and visualize accident hotspots across the U.S.

- Key Activities:<br/>
-Loaded and explored the US Accidents Dataset (March 2023)<br/>
-Cleaned the data by removing irrelevant columns and handling missing values<br/>
-Extracted hour information from timestamps for time-based analysis<br/>
-Analyzed the impact of road features like bumps, junctions, and traffic signals<br/>
-Identified top states and weather conditions linked to accidents<br/>
-Created a heatmap to highlight geographical accident hotspots<br/>

- Tools and Libraries used: Python, pandas, seaborn, matplotlib, folium

- Key Visuals Created:<br/>
ğŸ“Š Accidents by road features (Bump, Junction, Traffic Signal, etc.)<br/>
ğŸ“Š Top 10 states by number of accidents<br/>
ğŸ“Š Weather conditions most associated with accidents<br/>
ğŸ“Š Visibility vs severity comparison<br/>
ğŸ“Š Accident frequency by hour of day<br/>
ğŸ“ Interactive heatmap of accident locations using latitude and longitude<br/>

ğŸ“ˆ Outcome:<br/>
This task improved my skills in data cleaning, exploratory data analysis, and geospatial visualization. I learned how to extract insights from large datasets and identify real-world patterns in traffic accidents using visual tools.
